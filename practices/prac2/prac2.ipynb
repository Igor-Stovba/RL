{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aA14kYCYSwo"
   },
   "source": [
    "## Динамическое программирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roQ4hAstjVce"
   },
   "source": [
    "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
    "$$\n",
    "V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
    "$$\n",
    "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
    "$$\n",
    "Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
    "$$\n",
    "$$\n",
    "V_{(i+1)}(s) = \\max_a Q_i(s,a)\n",
    "$$\n",
    "\n",
    "Зададим напрямую модель MDP с картинки:\n",
    "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l6LwgNvgYXIP"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !wget https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OpKyGJEJYYDn"
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's2': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "import numpy as np\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVSC6KXuYcsh"
   },
   "source": [
    "Теперь мы можем использовать это MDP, как и любое другое gym окружение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PzLyFJ4iYfro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state =s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "state = mdp.reset()\n",
    "print('initial state =', state)\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print(f'next_state ={next_state}, reward = {reward}, done = {done}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgRdVPJlYjZ4"
   },
   "source": [
    ":Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4zK1xXedYn21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_states = ('s0', 's1', 's2')\n",
      "possible_actions('s1') =  ('a0', 'a1')\n",
      "next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "reward('s1', 'a0', 's0') =  5\n",
      "transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"all_states =\", mdp.get_all_states())\n",
    "print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"transition_prob('s1', 'a0', 's0') = \",\n",
    "      mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oe_RzZtYq11"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    "\n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n",
    "\n",
    "---\n",
    "\n",
    "Вначале вычисляем оценку состояния-действия:\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aA0DQccjody"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Qt0o0MokYv0F"
   },
   "outputs": [],
   "source": [
    "def get_action_value(\n",
    "    mdp, state_values, state, action, gamma\n",
    "):\n",
    "    \"\"\" Вычисляем Q(s,a) по формуле выше \"\"\"\n",
    "    # вычислеяем оценку состояния\n",
    "    Q = 0.0\n",
    "    all_states = mdp.get_all_states()\n",
    "    \n",
    "    for next_state in all_states:\n",
    "        Q += (mdp.get_transition_prob(state, action, next_state) \n",
    "                  * (mdp.get_reward(state, action, next_state) + gamma * state_values[next_state]))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "x06WscSIYysp"
   },
   "outputs": [],
   "source": [
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87q6GhsMY19h"
   },
   "source": [
    "Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O3QFuoVj1iZ"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hFqCuRaBY5J_"
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    return np.max(\n",
    "        np.array(\n",
    "            [get_action_value(mdp, state_values, state, action, gamma) for action in actions]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lPUyRzQOY8PP"
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
    "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
    "   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
    "assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od-SBiPKY_Q3"
   },
   "source": [
    "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-0-PlvmkF7P"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uUwb5JCDZDD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 3.50000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.64500 | V(start): 0.000 \n",
      "iter    2 | diff: 0.58050 | V(start): 0.581 \n",
      "iter    3 | diff: 0.43582 | V(start): 0.866 \n",
      "iter    4 | diff: 0.30634 | V(start): 1.145 \n",
      "iter    5 | diff: 0.27571 | V(start): 1.421 \n",
      "iter    6 | diff: 0.24347 | V(start): 1.655 \n",
      "iter    7 | diff: 0.21419 | V(start): 1.868 \n",
      "iter    8 | diff: 0.19277 | V(start): 2.061 \n",
      "iter    9 | diff: 0.17327 | V(start): 2.233 \n",
      "iter   10 | diff: 0.15569 | V(start): 2.389 \n",
      "iter   11 | diff: 0.14012 | V(start): 2.529 \n",
      "iter   12 | diff: 0.12610 | V(start): 2.655 \n",
      "iter   13 | diff: 0.11348 | V(start): 2.769 \n",
      "iter   14 | diff: 0.10213 | V(start): 2.871 \n",
      "iter   15 | diff: 0.09192 | V(start): 2.963 \n",
      "iter   16 | diff: 0.08272 | V(start): 3.045 \n",
      "iter   17 | diff: 0.07445 | V(start): 3.120 \n",
      "iter   18 | diff: 0.06701 | V(start): 3.187 \n",
      "iter   19 | diff: 0.06031 | V(start): 3.247 \n",
      "iter   20 | diff: 0.05428 | V(start): 3.301 \n",
      "iter   21 | diff: 0.04885 | V(start): 3.350 \n",
      "iter   22 | diff: 0.04396 | V(start): 3.394 \n",
      "iter   23 | diff: 0.03957 | V(start): 3.434 \n",
      "iter   24 | diff: 0.03561 | V(start): 3.469 \n",
      "iter   25 | diff: 0.03205 | V(start): 3.502 \n",
      "iter   26 | diff: 0.02884 | V(start): 3.530 \n",
      "iter   27 | diff: 0.02596 | V(start): 3.556 \n",
      "iter   28 | diff: 0.02336 | V(start): 3.580 \n",
      "iter   29 | diff: 0.02103 | V(start): 3.601 \n",
      "iter   30 | diff: 0.01892 | V(start): 3.620 \n",
      "iter   31 | diff: 0.01703 | V(start): 3.637 \n",
      "iter   32 | diff: 0.01533 | V(start): 3.652 \n",
      "iter   33 | diff: 0.01380 | V(start): 3.666 \n",
      "iter   34 | diff: 0.01242 | V(start): 3.678 \n",
      "iter   35 | diff: 0.01117 | V(start): 3.689 \n",
      "iter   36 | diff: 0.01006 | V(start): 3.699 \n",
      "iter   37 | diff: 0.00905 | V(start): 3.708 \n",
      "iter   38 | diff: 0.00815 | V(start): 3.717 \n",
      "iter   39 | diff: 0.00733 | V(start): 3.724 \n",
      "iter   40 | diff: 0.00660 | V(start): 3.731 \n",
      "iter   41 | diff: 0.00594 | V(start): 3.736 \n",
      "iter   42 | diff: 0.00534 | V(start): 3.742 \n",
      "iter   43 | diff: 0.00481 | V(start): 3.747 \n",
      "iter   44 | diff: 0.00433 | V(start): 3.751 \n",
      "iter   45 | diff: 0.00390 | V(start): 3.755 \n",
      "iter   46 | diff: 0.00351 | V(start): 3.758 \n",
      "iter   47 | diff: 0.00316 | V(start): 3.762 \n",
      "iter   48 | diff: 0.00284 | V(start): 3.764 \n",
      "iter   49 | diff: 0.00256 | V(start): 3.767 \n",
      "iter   50 | diff: 0.00230 | V(start): 3.769 \n",
      "iter   51 | diff: 0.00207 | V(start): 3.771 \n",
      "iter   52 | diff: 0.00186 | V(start): 3.773 \n",
      "iter   53 | diff: 0.00168 | V(start): 3.775 \n",
      "iter   54 | diff: 0.00151 | V(start): 3.776 \n",
      "iter   55 | diff: 0.00136 | V(start): 3.778 \n",
      "iter   56 | diff: 0.00122 | V(start): 3.779 \n",
      "iter   57 | diff: 0.00110 | V(start): 3.780 \n",
      "iter   58 | diff: 0.00099 | V(start): 3.781 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(\n",
    "    mdp, state_values=None,\n",
    "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5\n",
    "):\n",
    "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
    "    # инициализируем V(s)\n",
    "    state_values = state_values or \\\n",
    "    {s : 0 for s in mdp.get_all_states()}\n",
    "    new_state_values = state_values.copy()\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # Вычисляем новые полезности состояний,\n",
    "        # используя функции, определенные выше.\n",
    "        # Должен получиться словарь {s: new_V(s)}\n",
    "        for state in state_values.keys():\n",
    "            new_state_values[state] = get_new_state_value(mdp, state_values, state, gamma)        \n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Считаем разницу\n",
    "        diff = max(\n",
    "            abs(new_state_values[s] - state_values[s])\n",
    "            for s in mdp.get_all_states()\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"iter {i:4} | diff: {diff:6.5f} \"\n",
    "            f\"| V(start): {new_state_values[mdp._initial_state]:.3f} \"\n",
    "        )\n",
    "\n",
    "        state_values = new_state_values.copy()\n",
    "        if diff < min_difference:\n",
    "            print(\"Принято! Алгоритм сходится!\")\n",
    "            break\n",
    "\n",
    "    return state_values\n",
    "\n",
    "state_values = value_iteration(\n",
    "    mdp, num_iter = 100, min_difference = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZKomkPSrZGlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state values: {'s0': 3.7810348735476405, 's1': 7.294006423867229, 's2': 4.202140275227048}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 3.781) < 0.01\n",
    "assert abs(state_values['s1'] - 7.294) < 0.01\n",
    "assert abs(state_values['s2'] - 4.202) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gz5JxncZJoX"
   },
   "source": [
    "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
    "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml9AWeYrkNgf"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7gd4m26TZOn3"
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(\n",
    "    mdp, state_values, state, gamma=0.9\n",
    "):\n",
    "    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    # выбираем лучшее действие\n",
    "    i = np.argmax(\n",
    "        np.array(\n",
    "            [get_action_value(mdp, state_values, state, action, gamma) for action in actions]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yv3MRqaQZSzs"
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n",
    "\n",
    "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
    "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
    "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
    "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "V1KMZyhbZVVX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.4803\n"
     ]
    }
   ],
   "source": [
    "# Проверим среднее вознаграждение агента\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkokwmulZYYn"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E4V34IMzZbeH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "iter    0 | diff: 1.00000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.90000 | V(start): 0.000 \n",
      "iter    2 | diff: 0.81000 | V(start): 0.000 \n",
      "iter    3 | diff: 0.72900 | V(start): 0.000 \n",
      "iter    4 | diff: 0.65610 | V(start): 0.000 \n",
      "iter    5 | diff: 0.59049 | V(start): 0.590 \n",
      "iter    6 | diff: 0.00000 | V(start): 0.590 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()\n",
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNPtPQo2ZdpV"
   },
   "source": [
    "Визуализируем нашу стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aQP4HnjNZg4C"
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values, gamma=0.9):\n",
    "    \"\"\"функция визуализации стратегии\"\"\"\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {\n",
    "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
    "        for s in states\n",
    "    }\n",
    "    plt.imshow(\n",
    "        V.reshape(w, h),\n",
    "        cmap='gray', interpolation='none',\n",
    "        clim=(0, 1)\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h) - .5)\n",
    "    ax.set_yticks(np.arange(w) - .5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
    "            'right': (1, 0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,\n",
    "                     verticalalignment='center',\n",
    "                     horizontalalignment='center',\n",
    "                     fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u * .3, -v * .3,\n",
    "                      color='m', head_width=0.1,\n",
    "                      head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UJ2zkkx2Zlec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 29\n",
      "iter    0 | diff: 0.00000 | V(start): 0.198 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp,\n",
    "                            state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nltqOBDfZoFG"
   },
   "source": [
    "Посмотрим на оптимальную стратегию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CKJ1oJapZq77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "S*FFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SF*FFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFF*FFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFF*FFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFF*FF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFF*F\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFF*\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFF*\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF*\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF*\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF*\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH*\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH*\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, 0.9)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksq-NonlZtHM"
   },
   "source": [
    "Тестируем на более сложном варианте окружения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6g4fbKkkZza"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yOBqWNBfZv6v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 0.80000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.57600 | V(start): 0.000 \n",
      "iter    2 | diff: 0.41472 | V(start): 0.000 \n",
      "iter    3 | diff: 0.29860 | V(start): 0.000 \n",
      "iter    4 | diff: 0.24186 | V(start): 0.000 \n",
      "iter    5 | diff: 0.19349 | V(start): 0.000 \n",
      "iter    6 | diff: 0.15325 | V(start): 0.000 \n",
      "iter    7 | diff: 0.12288 | V(start): 0.000 \n",
      "iter    8 | diff: 0.09930 | V(start): 0.000 \n",
      "iter    9 | diff: 0.08037 | V(start): 0.000 \n",
      "iter   10 | diff: 0.06426 | V(start): 0.000 \n",
      "iter   11 | diff: 0.05129 | V(start): 0.000 \n",
      "iter   12 | diff: 0.04330 | V(start): 0.000 \n",
      "iter   13 | diff: 0.03802 | V(start): 0.033 \n",
      "iter   14 | diff: 0.03332 | V(start): 0.058 \n",
      "iter   15 | diff: 0.02910 | V(start): 0.087 \n",
      "iter   16 | diff: 0.01855 | V(start): 0.106 \n",
      "iter   17 | diff: 0.01403 | V(start): 0.120 \n",
      "iter   18 | diff: 0.00810 | V(start): 0.128 \n",
      "iter   19 | diff: 0.00555 | V(start): 0.133 \n",
      "iter   20 | diff: 0.00321 | V(start): 0.137 \n",
      "iter   21 | diff: 0.00247 | V(start): 0.138 \n",
      "iter   22 | diff: 0.00147 | V(start): 0.139 \n",
      "iter   23 | diff: 0.00104 | V(start): 0.140 \n",
      "iter   24 | diff: 0.00058 | V(start): 0.140 \n",
      "iter   25 | diff: 0.00036 | V(start): 0.141 \n",
      "iter   26 | diff: 0.00024 | V(start): 0.141 \n",
      "iter   27 | diff: 0.00018 | V(start): 0.141 \n",
      "iter   28 | diff: 0.00012 | V(start): 0.141 \n",
      "iter   29 | diff: 0.00007 | V(start): 0.141 \n",
      "iter   30 | diff: 0.00004 | V(start): 0.141 \n",
      "iter   31 | diff: 0.00003 | V(start): 0.141 \n",
      "iter   32 | diff: 0.00001 | V(start): 0.141 \n",
      "iter   33 | diff: 0.00001 | V(start): 0.141 \n",
      "Принято! Алгоритм сходится!\n",
      "Cреднее вознаграждение: 0.748\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    current_state = (0, 0) # initial state in lake \n",
    "    for t in range(100):\n",
    "        # выполняем оптимальное действие в окружении\n",
    "        optimal_action = get_optimal_action(mdp, state_values, current_state, 0.9)\n",
    "        next_state, r, done, _ = mdp.step(optimal_action)  \n",
    "        current_state = next_state\n",
    "\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Принято!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOEo-OheZyYP"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// случайно`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Считаем функцию $V^{\\pi_{n}}$\n",
    "- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n",
    "- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "PI включает в себя оценку полезности состояния, как внутренний шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diaeh1f7Z010"
   },
   "source": [
    "Вначале оценим полезности, используя текущую стратегию:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pFDjkE2kfsY"
   },
   "source": [
    "### 3 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "-RpV4Yw8Z3bi"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
    "    :param policy: словарь состояние->действие {s : a}\n",
    "    :returns: словарь {state : V^pi(state)}\n",
    "    \"\"\"\n",
    "    states = mdp.get_all_states()\n",
    "    n = len(states)\n",
    "    A = np.zeros((n, n))\n",
    "    b = np.zeros(n)\n",
    "    \n",
    "    for i, state in enumerate(states):\n",
    "        action = None\n",
    "        if state in policy:\n",
    "            action = policy[state]\n",
    "            next_states = mdp.get_next_states(state, action)  # dict {s': p}\n",
    "        \n",
    "            A[i, i] = 1.0\n",
    "            for j, next_state in enumerate(states):\n",
    "                prob = next_states.get(next_state, 0.0)\n",
    "                A[i, j] -= gamma * prob\n",
    "                \n",
    "            b[i] = sum(\n",
    "                prob * mdp.get_reward(state, action, next_state)\n",
    "                for next_state, prob in next_states.items()\n",
    "            )\n",
    "        else:\n",
    "            A[i, i] = 1.0\n",
    "        \n",
    "    values = solve(A, b)\n",
    "\n",
    "    state_values = {\n",
    "        states[i] : values[i]\n",
    "        for i in range(len(states))\n",
    "    }\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "qeb79E20Z6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 2.8399699474079663, 's1': 6.498873027798649, 's2': 3.4710743801652932}\n"
     ]
    }
   ],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "test_policy = {\n",
    "    s: np.random.choice(mdp.get_possible_actions(s))\n",
    "    for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "assert type(new_vpi) is dict, \\\n",
    "    \"функция compute_vpi должна возвращать словарь \\\n",
    "    {состояние s : V^pi(s) }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du2YNXpxZ9BT"
   },
   "source": [
    "Теперь обновляем стратегию на основе новых значений полезностей:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsYlHPblkrSI"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "TGCHMeaUZ_Qj"
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Рассчитываем новую стратегию\n",
    "    :param vpi: словарь {state : V^pi(state) }\n",
    "    :returns: словарь {state : оптимальное действие}\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        Q[state] = {}\n",
    "        for a in mdp.get_possible_actions(state):\n",
    "            values = []\n",
    "            for next_state in mdp.get_next_states(state, a):\n",
    "                r = mdp.get_reward(state, a, next_state)\n",
    "                p = mdp.get_transition_prob(\n",
    "                    state, a, next_state\n",
    "                )\n",
    "                values.append(p * (r + gamma * vpi[next_state]))\n",
    "\n",
    "            Q[state][a] = sum(values)\n",
    "\n",
    "    policy = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        actions = mdp.get_possible_actions(state)\n",
    "        if actions:\n",
    "            # выбираем оптимальное действие в state\n",
    "            policy[state] = get_optimal_action(mdp, vpi, state, gamma)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "1b1OXlg9aBsy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a1', 's1': 'a0', 's2': 'a0'}\n"
     ]
    }
   ],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(new_policy) is dict, \\\n",
    "\"функция compute_new_policy должна возвращать словарь \\\n",
    "{состояние s: оптимальное действие}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15mJglOZaEmI"
   },
   "source": [
    "Собираем все в единый цикл:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIrgcIqKkxD2"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "2LcLHHhIaHAZ"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    mdp, policy=None, gamma = 0.9,\n",
    "    num_iter = 1000, min_difference = 1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Запускаем цикл итерации по стратегиям\n",
    "    Если стратегия не определена, задаем случайную\n",
    "    \"\"\"\n",
    "    for i in range(num_iter):\n",
    "        if not policy:\n",
    "            policy = {}\n",
    "            for s in mdp.get_all_states():\n",
    "                if mdp.get_possible_actions(s):\n",
    "                    policy[s] = (\n",
    "                        np.random.choice(mdp.get_possible_actions(s))\n",
    "                    )\n",
    "        state_values = compute_vpi(mdp, policy, gamma)\n",
    "        policy = compute_new_policy(mdp, state_values, gamma)\n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddfLTSfgaJjU"
   },
   "source": [
    "Тестируем на FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "4hLv3X0OaKmg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE: (0, 0)\n",
      "STATE: (0, 1)\n",
      "STATE: (0, 2)\n",
      "STATE: (0, 3)\n",
      "STATE: (1, 0)\n",
      "STATE: (1, 2)\n",
      "STATE: (2, 0)\n",
      "STATE: (2, 1)\n",
      "STATE: (2, 2)\n",
      "STATE: (3, 1)\n",
      "STATE: (3, 2)\n",
      "average reward:  0.86\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values, policy = policy_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(policy[s])\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Принято!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "dl_hw1_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
